import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import joblib
import random

def in_progress_generator(n_exemplos):
    games = []
    while len(games) < n_exemplos:
        board = [0] * 9
        plays = random.randint(2, 8)
        positions = random.sample(range(9), plays)
        players = 1
        for i in positions:
            board[i] = players
            players *= -1  # Flow between 1 and -1
        if not simple_win_check(np.array(board).reshape(3, 3)):
            games.append(board + ['in_progress'])
    return pd.DataFrame(games, columns=[f'pos{i}' for i in range(1, 10)] + ['class'])

def simple_win_check(tab):
    for i in range(3):
        if abs(tab[i, :].sum()) == 3 or abs(tab[:, i].sum()) == 3:
            return True
    if abs(np.diag(tab).sum()) == 3 or abs(np.diag(np.fliplr(tab)).sum()) == 3:
        return True
    return False

# 1. Load the dataset
dataset_url = "https://archive.ics.uci.edu/ml/machine-learning-databases/tic-tac-toe/tic-tac-toe.data"
columns = [f'pos{i}' for i in range(1, 10)] + ['class']
df = pd.read_csv(dataset_url, names=columns)

# 2. Map values
mapping = {'x': 1, 'o': -1, 'b': 0}
for col in columns[:-1]:
    df[col] = df[col].map(mapping)

print("Original class distribution:")
print(df['class'].value_counts())

# 3. Create in progress examples and merge with the original dataset
min_class_count = df['class'].value_counts().min()
df_in_progress = in_progress_generator(min_class_count)
df_total_raw = pd.concat([df, df_in_progress], ignore_index=True)

# 4. Balance the dataset
min_total_class_count = df_total_raw['class'].value_counts().min()
df_total = pd.concat([
    df_total_raw[df_total_raw['class'] == cls].sample(n=min_total_class_count, random_state=42)
    for cls in df_total_raw['class'].unique()
]).reset_index(drop=True)

# 5. Split into X and y
X = df_total[columns[:-1]]
y = df_total['class']

# 6. Split into train, val, test (60/20/20)
X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp)  # 0.25 x 0.8 = 0.2

print(f"Train size: {len(X_train)}, Val size: {len(X_val)}, Test size: {len(X_test)}")
print("Balanced class distribution in train set:")
print(y_train.value_counts())

# 7. Train models with GridSearchCV
# KNN
knn = KNeighborsClassifier()
knn_param_grid = {
    'n_neighbors': [3, 5, 7],
    'weights': ['uniform', 'distance']
}
knn_grid = GridSearchCV(knn, knn_param_grid, cv=5)
knn_grid.fit(X_train, y_train)
knn_best = knn_grid.best_estimator_

# MLP
mlp = MLPClassifier(max_iter=2000, random_state=42, early_stopping=True)
mlp_param_grid = {
    'hidden_layer_sizes': [(50,), (100,)],
    'activation': ['relu', 'tanh'],
    'learning_rate_init': [0.001, 0.01]
}
mlp_grid = GridSearchCV(mlp, mlp_param_grid, cv=5)
mlp_grid.fit(X_train, y_train)
mlp_best = mlp_grid.best_estimator_

# Decision Tree
dt = DecisionTreeClassifier(random_state=42)
dt_param_grid = {
    'max_depth': [3, 5, 7],
    'min_samples_split': [2, 5, 10]
}
dt_grid = GridSearchCV(dt, dt_param_grid, cv=5)
dt_grid.fit(X_train, y_train)
dt_best = dt_grid.best_estimator_

# Random Forest
rf = RandomForestClassifier(random_state=42)
rf_param_grid = {
    'n_estimators': [50, 100],
    'max_depth': [5, 10]
}
rf_grid = GridSearchCV(rf, rf_param_grid, cv=5)
rf_grid.fit(X_train, y_train)
rf_best = rf_grid.best_estimator_

print("Modelos treinados e melhores hiperparâmetros encontrados!")

# 8. Comparação dos modelos
models = {
    "KNN": knn_best,
    "MLP": mlp_best,
    "Decision Tree": dt_best,
    "Random Forest": rf_best
}

results = []
for name, model in models.items():
    y_pred = model.predict(X_val)
    results.append({
        "Model": name,
        "Accuracy": accuracy_score(y_val, y_pred),
        "Precision": precision_score(y_val, y_pred, average='macro'),
        "Recall": recall_score(y_val, y_pred, average='macro'),
        "F1": f1_score(y_val, y_pred, average='macro')
    })

results_df = pd.DataFrame(results)
print("\nComparação dos modelos no conjunto de validação:")
print(results_df)

# 9. Escolha o melhor modelo (exemplo: rf_best, mas pode ser outro)
melhor_modelo = mlp_best  # Troque para knn_best, mlp_best, dt_best

# 10. Salve o modelo
joblib.dump(melhor_modelo, 'meu_modelo_treinado.pkl')
print('Modelo salvo em meu_modelo_treinado.pkl') 